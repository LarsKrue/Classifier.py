"""
Created on Mon May 25 19:42:06 2020

@author: Lars Kr√ºger
"""
# identify dog photos from the Kaggle dogs vs cats dataset

from matplotlib.image import imread
import matplotlib.pyplot as plt
from torch import nn
import torch.nn.functional as F
import helpers
from torchvision import datasets, transforms
import torch as tr
from torch import optim
import time
import numpy as np



#---------------------------------------------------------------------------
start_time = time.time() 
#-----------------------------------------------------------------------------

image_height = 224
crop = 224
angle_rot = 30

# Define a transform pipeline to preprocess image data train
train_transform = transforms.Compose([transforms.RandomRotation(angle_rot),# returns PIL image
                                transforms.RandomResizedCrop(crop),
                                transforms.Resize(image_height),
                                transforms.Grayscale(), # returns PIL image
                                transforms.ToTensor(), # returns tensor
                                transforms.Normalize((0.5,), (0.5,)), # expects tensor                                
                                ])

# Define a transform pipeline to preprocess image data test
test_transform = transforms.Compose([transforms.Resize(image_height),# returns PIL image
                                transforms.CenterCrop(crop), # returns PIL image
                                transforms.Resize(image_height),
                                transforms.Grayscale(), # returns PIL image
                                transforms.ToTensor(), # returns tensor
                                transforms.Normalize((0.5,), (0.5,)), # expects tensor                                
                                ])



# Load the training data into loader object
train_set = datasets.ImageFolder(root="img/train/", transform=train_transform)
train_loader = tr.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)

# Load the test data into loader object
test_set = datasets.ImageFolder(root="img/test/", transform=test_transform)
test_loader = tr.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)

# Initialize Neural Network
n_input =  image_height**2  # total number of features of input / 40 x 40 pixels
n_hidden = [512, 256]       # sizes of hidden layers /  len[list] number of hidden layers
n_output = 1                # number of categories of classifier


#Build feed-forward NN / order important, sequential pipeline
my_network = nn.Sequential(
                    	   nn.Linear(n_input,n_hidden[0]),
                           nn.ReLU(),
                           nn.Dropout(0.2),  #randomly drop input units to prevent overfitting
                           nn.Linear(n_hidden[0],n_hidden[1]),
                           nn.ReLU(),
                           nn.Linear(n_hidden[1],n_output)
                           ) #no sigmoid

# Define loss function called criterion
criterion = nn.BCEWithLogitsLoss() # 

# Optimizers require the network's parameters to optimize and a learning rate
optimizer = optim.Adam(my_network.parameters(), lr=0.01)


#--------------------------- Training NN -------------------------------------


epochs = 3
steps = 0
train_losses, test_losses, accuracy_list = [], [], []

for loop in range(epochs):
    running_loss = 0
    print("Start Training")
   
    #-----------------------Training-----------------------------------
    for images, labels in train_loader:
        
        # Flatten MNIST images 
        images = images.view(images.shape[0], -1)
        
        # Probability Distribution  / output
        logps = my_network.forward(images)
        
        # adjustment
        logps = tr.squeeze(logps)
        labels = labels.float()
        
        
        #Loss Function
        loss = criterion(logps, labels)
        running_loss += loss.item()
        
        # clear gradients
        optimizer.zero_grad()
        
        
        # Backward pass / Back Propagation
        loss.backward()
        
        #Update Weights
        optimizer.step()
        
    else: # for else loop, if no break / loop terminated normally, then Else
        test_loss = 0
        accuracy = 0
        print("Start Test")
        #-----------------------Test Cycle-----------------------------------
        
        # turn-off gradient calculation for validation, saves memory + computations
        with tr.no_grad():
            for images, labels in test_loader:
                
                # Flatten Input vector
                images = images.view(images.shape[0], -1)
                
                # Forward Loop
                logps=my_network(images)
                labels = tr.unsqueeze(labels, dim=1)
                labels = labels.float()
                test_loss += criterion(logps, labels)
                
                
                # accuracy calculations
                ps = tr.exp(logps)
                top_p, top_class = ps.topk(1, dim=1)
                equals = top_class == labels.view(*top_class.shape) # True where identity
                accuracy += tr.mean(equals.type(tr.FloatTensor))
                
        train_losses.append(running_loss/len(train_loader))
        test_losses.append(test_loss/len(test_loader))
        accuracy_list.append(accuracy/len(test_loader))
        
        print("After Epoch {} of {}".format(loop+1, epochs))
        print("Training Error {}".format(running_loss/len(train_loader)))
        print("Test Error {}".format(test_loss/len(test_loader)))
        print("Test Accuracy {}".format(accuracy/len(test_loader)))


# --------------------overfitting---------------------------------------------
# training loss decreases, validation loss increases over time


# Plot network performance
plt.plot(range(0,epochs), train_losses, label='Training Error')
plt.plot(range(0,epochs), test_losses, label='Test Error')
plt.legend(frameon=False)
plt.show()
plt.bar(range(0,epochs),accuracy_list)
plt.show()

#-------------------------Save- network---------------------------------------

# all architecture information needed!
checkpoint = {"input_size": n_input,
              "output_size" : n_output,              
              "state_dict" : my_network.state_dict()
              }
tr.save(checkpoint ,"checkpoint.pth")

#------------------------------Time-------------------------------------------
end_time = time.time()
tot_time = end_time - start_time 
print("\n** Total Elapsed Runtime:",
      str(int((tot_time/3600)))+":"+str(int((tot_time%3600)/60))+":"
      +str(int((tot_time%3600)%60)) )
    
#----------------------- Inference--------------------------------------------

my_network.eval() # inference mode

images, labels = next(iter(test_loader)) # shuffle enables random images
img = images[0]
img = img.view(1,image_height**2) # flatten image vector

with tr.no_grad(): # no gradient calculation
    logps = my_network.forward(img)
    #ps = tr.exp(logps)
    print("Probability", logps)
    print("Label", labels[0])

#-----------------------------Plot--------------------------------------------
ps = tr.exp(logps)
print("ps after Exp", ps)

fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)
ax1.imshow(img.resize_(1, image_height, image_height).numpy().squeeze())
ax1.axis('off')
ax2.barh(np.arange(2),[ps, 1-ps])
ax2.set_aspect(0.1)
ax2.set_yticks(np.arange(2))
ax2.set_yticklabels(["Cat", "Dog"])
ax2.set_title('Class Probability')
